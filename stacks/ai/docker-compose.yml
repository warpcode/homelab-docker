services:

  model-tinyllama:
    image: ghcr.io/ggerganov/llama.cpp:server-vulkan
    devices:
      - /dev/dri:/dev/dri
    command: [
      # "-m", "/models/tinyllama/tinyllama-1.1b-chat-v1.0.q4_k_m.gguf",
      "-hf", "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF:Q4_K_M",
      "--gpu-layers", "999",
      "--host", "0.0.0.0",
      "--port", "8080",
      # "-n", "150",
      "--repeat-penalty", "1.3",
      "--repeat-last-n", "256",
      "--temp", "0.6",
      "--top-p", "0.9",
      "--ctx-size", "2048"
    ]
    # networks:
    #   - proxy_proxy
    labels:
      - "deunhealth.restart.on.unhealthy=true"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    # ports:
    #   - "4000:4000" # Map the container port to the host, change the host port if necessary
    environment:
      - LITELLM_LOG=INFO
    env_file:
      - .env
    configs:
      - source: litellm-config
        target: /config/config.yml
      - source: litellm-prompts
        target: /prompts/
    # You can change the port or number of workers as per your requirements or pass any new supported CLI argument. Make sure the port passed here matches with the container port defined above in `ports` value
    command:
      - "--config"
      - "/config/config.yml"
      - "--port"
      - "4000"
      - "--num_workers"
      - "8"
    networks:
      - default
      - proxy_proxy
    labels:
      - "deunhealth.restart.on.unhealthy=true"
      - "traefik.enable=true"
      # http
      - "traefik.http.routers.litellm.entrypoints=web"
      - "traefik.http.routers.litellm.rule=Host(`litellm.ai.${BASE_HOSTNAME:-localhost}`)"
      - "traefik.http.routers.litellm.service=litellm"
      - "traefik.http.services.litellm.loadbalancer.server.port=4000"
      # - "traefik.http.routers.litellm.middlewares=litellm-https-redirect"
      # - "traefik.http.middlewares.litellm-https-redirect.redirectscheme.scheme=https"
      # https
      # - "traefik.http.routers.litellm-secure.entrypoints=websecure"
      # - "traefik.http.routers.litellm-secure.rule=Host(`litellm.ai.${BASE_HOSTNAME:-localhost}`)"
      # - "traefik.http.routers.litellm-secure.tls.certresolver=leresolver"
      # - "traefik.http.routers.litellm-secure.service=litellm-secure"
      # - "traefik.http.services.litellm-secure.loadbalancer.server.port=4000"



  # ollama:
  #   entrypoint: ["/entrypoint.sh"]
  #   volumes:
  #     - ./data/ollama-config/ollama:/root/.ollama
  #     - ./entrypoint.sh:/entrypoint.sh:ro
  #   ports:
  #     - 11434:11434
  #   container_name: ollama
  #   image: ollama/ollama:rocm
  #   #image: ollama/ollama:latest #nvidia
  #   hostname: ollama
  #   devices:
  #     - /dev/dri
  #     - /dev/kfd
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - capabilities: ["gpu"]
  #   labels:
  #     deunhealth.restart.on.unhealthy: true
  #   # healthcheck:
  #   #   test: ["CMD", "sh", "-c", "wget -q --spider http://localhost:11434 || exit 1"]
  #   #   interval: 10s
  #   #   retries: 3
  #   #   start_period: 5s
  #   #   timeout: 5s

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    # volumes:
    #   - ./data/open-webui:/app/backend/data
    # depends_on:
    #   ollama:
    #     condition: service_healthy
    # ports:
    #   - 8282:8080
    environment:
      - ENABLE_OPENAI_API=true
      - OPENAI_API_BASE_URLS=http://litellm:4000/v1
      - OPENAI_API_KEYS=${LITELLM_MASTER_KEY:-sk-1234}
      # - OLLAMA_BASE_URL=http://ollama:11434
      # - WEBUI_SECRET_KEY: testtest
    env_file:
      - .env
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped
    networks:
      - default
      - proxy_proxy
    labels:
      - "deunhealth.restart.on.unhealthy=true"
      - "traefik.enable=true"
      - "traefik.docker.network=proxy_proxy"
      # http
      - "traefik.http.routers.ai.entrypoints=web"
      - "traefik.http.routers.ai.rule=Host(`ai.${BASE_HOSTNAME:-localhost}`)"
      - "traefik.http.routers.ai.middlewares=ai-https-redirect"
      - "traefik.http.middlewares.ai-https-redirect.redirectscheme.scheme=https"
      # https
      - "traefik.http.routers.ai-secure.entrypoints=websecure"
      - "traefik.http.routers.ai-secure.rule=Host(`ai.${BASE_HOSTNAME:-localhost}`)"
      - "traefik.http.routers.ai-secure.tls.certresolver=leresolver"
      - "traefik.http.routers.ai-secure.service=ai-secure"
      - "traefik.http.services.ai-secure.loadbalancer.server.port=8080"

  # gpt4free:
  #   image: hlohaus789/g4f
  #   container_name: gpt4free
  #   ports:
  #     - 8080:8080
  #     - 1337:1337
  #     - 7900:7900
  #   restart: unless-stopped
  #   networks:
  #     - proxy_proxy

  # private-gpt:
  #   image: zylonai/private-gpt:0.6.2-ollama
  #   user: root
  #   volumes:
  #     - ./data/private-gpt:/home/worker/app/local_data
  #   ports:
  #     - "8001:8001"
  #   environment:
  #     PORT: 8001
  #     PGPT_PROFILES: docker
  #     PGPT_MODE: ollama
  #     PGPT_EMBED_MODE: ollama
  #     PGPT_OLLAMA_API_BASE: http://ollama:11434
  #     HF_TOKEN: ""
  #   # profiles:
  #   #   - ""
  #   #   - ollama-cpu
  #   #   - ollama-cuda
  #   #   - ollama-api
  #   depends_on:
  #     - ollama

configs:
  litellm-config:
    file: ./assets/litellm-config.yml
  litellm-prompts:
    file: ./assets/prompts/

networks:
  proxy_proxy:
    external: true
