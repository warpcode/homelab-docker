services:
  # model-tinyllama:
  #   image: ghcr.io/ggerganov/llama.cpp:server-vulkan
  #   devices:
  #     - /dev/dri:/dev/dri
  #   command: "-hf TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF:Q4_K_M --gpu-layers 999 --host 0.0.0.0 --port 8080 --repeat-penalty 1.3 --repeat-last-n 256 --temp 0.6 --top-p 0.9 --ctx-size 2048"
  #   # networks:
  #   #   - proxy_proxy
  #   labels:
  #     - "deunhealth.restart.on.unhealthy=true"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 60s

  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    # ports:
    #   - "4000:4000" # Map the container port to the host, change the host port if necessary
    environment:
      - LITELLM_LOG=INFO
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GITHUB_API_KEY=${GITHUB_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
    configs:
      - source: litellm-config
        target: /config/config.yml
    # You can change the port or number of workers as per your requirements or pass any new supported CLI argument. Make sure the port passed here matches with the container port defined above in `ports` value
    command:
      - "--config"
      - "/config/config.yml"
      - "--port"
      - "4000"
      - "--num_workers"
      - "8"
    deploy:
      restart_policy:
        condition: on-failure
    networks:
      - default
      - proxy_proxy
    labels:
        - "deunhealth.restart.on.unhealthy=true"
        - "traefik.enable=true"
        - "traefik.docker.network=proxy_proxy"
        # http
        - "traefik.http.routers.litellm.entrypoints=web"
        - 'traefik.http.routers.litellm.rule=Host(`litellm.ai.${BASE_HOSTNAME:-localhost}`)'
        - "traefik.http.routers.litellm.service=litellm"
        - "traefik.http.services.litellm.loadbalancer.server.port=4000"
        # - "traefik.http.routers.litellm.middlewares=litellm-https-redirect"
        # - "traefik.http.middlewares.litellm-https-redirect.redirectscheme.scheme=https"
        # https
        # - "traefik.http.routers.litellm-secure.entrypoints=websecure"
        # - "traefik.http.routers.litellm-secure.rule=Host(litellm.ai.${BASE_HOSTNAME:-localhost})"
        # - "traefik.http.routers.litellm-secure.tls.certresolver=leresolver"
        # - "traefik.http.routers.litellm-secure.service=litellm-secure"
        # - "traefik.http.services.litellm-secure.loadbalancer.server.port=4000"



  # ollama:
  #   entrypoint: ["/entrypoint.sh"]
  #   volumes:
  #     - ./data/ollama-config/ollama:/root/.ollama
  #     - ./entrypoint.sh:/entrypoint.sh:ro
  #   ports:
  #     - 11434:11434
  #   container_name: ollama
  #   image: ollama/ollama:rocm
  #   #image: ollama/ollama:latest #nvidia
  #   hostname: ollama
  #   devices:
  #     - /dev/dri
  #     - /dev/kfd
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - capabilities: ["gpu"]
  #   labels:
  #     deunhealth.restart.on.unhealthy: true
  #   # healthcheck:
  #   #   test: ["CMD", "sh", "-c", "wget -q --spider http://localhost:11434 || exit 1"]
  #   #   interval: 10s
  #   #   retries: 3
  #   #   start_period: 5s
  #   #   timeout: 5s

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    # volumes:
    #   - ./data/open-webui:/app/backend/data
    # ports:
    #   - 8282:8080
    environment:
      - ENABLE_OPENAI_API=true
      # - OPENAI_API_BASE_URLS=http://litellm:4000/v1;http://host.docker.internal:12434/engines/llama.cpp/v1
      - OPENAI_API_BASE_URLS=http://litellm.ai.warpcode.co.uk/v1
      - OPENAI_API_KEYS=${LITELLM_MASTER_KEY:-sk-1234}
      # - OLLAMA_BASE_URL=http://ollama:11434
      # - WEBUI_SECRET_KEY: testtest
    extra_hosts:
      - host.docker.internal:host-gateway
    deploy:
      restart_policy:
        condition: on-failure
    networks:
      - default
      - proxy_proxy
    labels:
      - "deunhealth.restart.on.unhealthy=true"
      - "traefik.enable=true"
      - "traefik.docker.network=proxy_proxy"
      # http
      - "traefik.http.routers.ai.entrypoints=web"
      - 'traefik.http.routers.ai.rule=Host(`ai.${BASE_HOSTNAME:-localhost}`)'
      - "traefik.http.routers.ai.middlewares=ai-https-redirect"
      - "traefik.http.middlewares.ai-https-redirect.redirectscheme.scheme=https"
      # https
      - "traefik.http.routers.ai-secure.entrypoints=websecure"
      - 'traefik.http.routers.ai-secure.rule=Host(`ai.${BASE_HOSTNAME:-localhost}`)'
      - "traefik.http.routers.ai-secure.tls.certresolver=leresolver"
      - "traefik.http.routers.ai-secure.service=ai-secure"
      - "traefik.http.services.ai-secure.loadbalancer.server.port=8080"

  # gpt4free:
  #   image: hlohaus789/g4f
  #   container_name: gpt4free
  #   ports:
  #     - 8080:8080
  #     - 1337:1337
  #     - 7900:7900
  #   restart: unless-stopped
  #   networks:
  #     - proxy_proxy

  # private-gpt:
  #   image: zylonai/private-gpt:0.6.2-ollama
  #   user: root
  #   volumes:
  #     - ./data/private-gpt:/home/worker/app/local_data
  #   ports:
  #     - "8001:8001"
  #   environment:
  #     PORT: 8001
  #     PGPT_PROFILES: docker
  #     PGPT_MODE: ollama
  #     PGPT_EMBED_MODE: ollama
  #     PGPT_OLLAMA_API_BASE: http://ollama:11434
  #     HF_TOKEN: ""
  #   # profiles:
  #   #   - ""
  #   #   - ollama-cpu
  #   #   - ollama-cuda
  #   #   - ollama-api
  #   depends_on:
  #     - ollama
configs:
  litellm-config:
    file: ./config/litellm-config.yml
  # litellm-prompts:
  #   file: ./assets/prompts/

networks:
  proxy_proxy:
    external: true
